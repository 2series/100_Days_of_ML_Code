{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning: Intro to Monte Carlo Learning using OpenAI Gym Toolkit.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2series/100_Days_of_ML_Code/blob/master/Reinforcement_Learning_Intro_to_Monte_Carlo_Learning_using_OpenAI_Gym_Toolkit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fct2Lm6yX2Xd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo Methods \n",
        "Any method which solves a problem by generating suitable random numbers, and observing that fraction of numbers obeying some property or properties, can be classified as a Monte Carlo method.\n",
        "\n",
        "# Monte Carlo Reinforcement Learning\n",
        "The Monte Carlo method for reinforcement learning learns directly from episodes of experience without any prior knowledge of MDP transitions. Here, the random component is the return or reward."
      ]
    },
    {
      "metadata": {
        "id": "DZF250qxVtla",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This project I'll try to understand the basics of Monte Carlo learning. It’s used when there is no prior information of the environment and all the information is essentially collected by experience. I’ll use the OpenAI Gym toolkit in Python to implement this method as well."
      ]
    },
    {
      "metadata": {
        "id": "F2WvWPRtVtbI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Knowing that dynamic programming is used to solve problems where the underlying model of the environment is known beforehand (or more precisely, model-based learning). Reinforcement Learning is all about learning from experience in playing games.\n",
        "\n",
        "E.g. say we want to train a bot to learn how to play chess. Consider converting the chess environment into an Markov Decision Process (MDP).\n",
        "\n",
        "Now, depending on the positioning of pieces, this environment will have many states (more than 1050), as well as a large number of possible actions. The model of this environment is almost impossible to design!\n",
        "\n",
        "One potential solution could be to repeatedly play a complete game of chess and receive a positive reward for winning, and a negative reward for losing, at the end of each game. This is called learning from experience."
      ]
    },
    {
      "metadata": {
        "id": "Dv8lMFaYVtXG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "qTcyVSRqVtTU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "kC6olp2TVtLm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "RZUoUK7FRYD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "f4107b60-54bd-4579-c557-8635d40c1288"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/22/4ff09745ade385ffe707fb5f053548f0f6a6e7d5e98a2b9d6c07f5b931a7/gym-0.10.9.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 15.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Collecting pyglet>=1.2.0 (from gym)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 17.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.10.15)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6c/3a/0e/b86dee98876bb56cdb482cc1f72201035e46d1baf69d10d028\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "Successfully installed gym-0.10.9 pyglet-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jjT6aNsxZJCw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create the Environment"
      ]
    },
    {
      "metadata": {
        "id": "HoI16_XVQceY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import operator\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "import random\n",
        "import itertools\n",
        "import tqdm\n",
        "\n",
        "tqdm.monitor_interval = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zMoC-2P_ZP60",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Function for Random Policy"
      ]
    },
    {
      "metadata": {
        "id": "tpWPqb4qRDC1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_random_policy(env):\n",
        "     policy = {}\n",
        "     for key in range(0, env.observation_space.n):\n",
        "          current_end = 0\n",
        "          p = {}\n",
        "          for action in range(0, env.action_space.n):\n",
        "               p[action] = 1 / env.action_space.n\n",
        "          policy[key] = p\n",
        "     return policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mDeZO20qZZdp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dictionary for storing the state action value"
      ]
    },
    {
      "metadata": {
        "id": "wJ15Tp2JRC71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_state_action_dictionary(env, policy):\n",
        "    Q = {}\n",
        "    for key in policy.keys():\n",
        "         Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n",
        "    return Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yTg08gQQZf_Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Function to play an episode"
      ]
    },
    {
      "metadata": {
        "id": "7X97qZMERC0Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_game(env, policy, display=True):\n",
        "     env.reset()\n",
        "     episode = []\n",
        "     finished = False\n",
        "\n",
        "     while not finished:\n",
        "          s = env.env.s\n",
        "          if display:\n",
        "               clear_output(True)\n",
        "               env.render()\n",
        "               sleep(1)\n",
        "\n",
        "          timestep = []\n",
        "          timestep.append(s)\n",
        "          n = random.uniform(0, sum(policy[s].values()))\n",
        "          top_range = 0\n",
        "          for prob in policy[s].items():\n",
        "                 top_range += prob[1]\n",
        "                 if n < top_range:\n",
        "                       action = prob[0]\n",
        "                       break \n",
        "          state, reward, finished, info = env.step(action)\n",
        "          timestep.append(action)\n",
        "          timestep.append(reward)\n",
        "\n",
        "          episode.append(timestep)\n",
        "\n",
        "     if display:\n",
        "          clear_output(True)\n",
        "          env.render()\n",
        "          sleep(1)\n",
        "     return episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YwAtfC6SZoV3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Function to test policy and print win percentage"
      ]
    },
    {
      "metadata": {
        "id": "gdBJNOsjRCqp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_policy(policy, env):\n",
        "      wins = 0\n",
        "      r = 100\n",
        "      for i in range(r):\n",
        "            w = run_game(env, policy, display=False)[-1][-1]\n",
        "            if w == 1:\n",
        "                  wins += 1\n",
        "      return wins / r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hjbYqDtsZz6v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# First Visit Monte Carlo Prediction and Control"
      ]
    },
    {
      "metadata": {
        "id": "3vuMGDVfR0wn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
        "    if not policy:\n",
        "        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    \n",
        "    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n",
        "    returns = {} # 3.\n",
        "    \n",
        "    for _ in range(episodes): # Looping through episodes\n",
        "        G = 0 # Store cumulative reward in G (initialized at 0)\n",
        "        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively \n",
        "        \n",
        "        # for loop through reversed indices of episode array. \n",
        "        # The logic behind it being reversed is that the eventual reward would be at the end. \n",
        "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
        "        \n",
        "        for i in reversed(range(0, len(episode))):   \n",
        "            s_t, a_t, r_t = episode[i] \n",
        "            state_action = (s_t, a_t)\n",
        "            G += r_t # Increment total reward by reward on current timestep\n",
        "            \n",
        "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # \n",
        "                if returns.get(state_action):\n",
        "                    returns[state_action].append(G)\n",
        "                else:\n",
        "                    returns[state_action] = [G]   \n",
        "                    \n",
        "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n",
        "                \n",
        "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n",
        "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
        "                max_Q = random.choice(indices)\n",
        "                \n",
        "                A_star = max_Q # 14.\n",
        "                \n",
        "                for a in policy[s_t].items(): # Update action probability for s_t in policy\n",
        "                    if a[0] == A_star:\n",
        "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
        "                    else:\n",
        "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
        "\n",
        "    return policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_kdq6OWMaGeR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Now, it is time to run this algorithm to solve an 8×8 frozen lake environment and check the reward:"
      ]
    },
    {
      "metadata": {
        "id": "tAtWqObTR1V6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "1fa49429-8292-4639-d3d0-5ff1a6cc3b39"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake8x8-v0')\n",
        "policy = monte_carlo_e_soft(env, episodes=50000)\n",
        "test_policy(policy, env)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "Uwkf1PP_a9SD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# On running this for 50,000 episodes, we get a score of 0.32. And with more episodes, it eventually reaches the optimal policy."
      ]
    },
    {
      "metadata": {
        "id": "Xnk-_SLuR0l5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}